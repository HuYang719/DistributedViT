[net]
batch=1
height=384
width=384
channels=3
model_dim = 768
subdivisions = 1
scales=.01,.01


[convolutional]
batch_normalize=0
filters =768
size = 16
stride = 16
activation=linear

[flatten]
model_dim=768
gw=24
gh=24

[positional_embedding]
input_size=577
model_dim=768

# block 1

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

## state 9
[shortcut]
from=-4
c=577
h=12
w=64 




# # block 2
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 
  

# # block 3

## state 17                                              
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 

# # # block 4

## 24
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 

# # # block 5
## state 31

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 


# # # block 6
## state 38
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 


# # # block 7
## state 45
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 


# # # block 8
## state 52
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 

# # # block 9
## state 59
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 

# # # block 10
## state 66
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 

# # # # block 11
## state 73
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 


# # # # block 12
## state 80
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0

[shortcut]
from=-4
c=577
h=12
w=64 


# # OUTPUT

[layernorm]
input_size = 577
model_dim = 768
train = 0
cut = 1

# # [debug]
# # dim1 = 1
# # dim2 = 1
# # dim3 = 768


##state 88
[linear]
input_size = 1
model_dim = 768
output_size = 1000
activation = softmax
dropout = 0

[debug]
dim1 = 1
dim2 = 1
dim3 = 1000






