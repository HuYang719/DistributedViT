[net]
batch=1
height=384
width=384
channels=3
model_dim = 768
subdivisions = 1
scales=.01,.01


[convolutional]
batch_normalize=0
filters =768
size = 16
stride = 16
activation=linear

[flatten]
model_dim=768
gw=24
gh=24

[positional_embedding]
input_size=576
model_dim=768

# block 1

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 2
[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

# block 3

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 4

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 5

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 6
[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

# block 7

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 8

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 9
[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# block 10

# [debug]
# dim1 = 1
# dim2 = 576
# dim3 = 768  

[layernorm]
input_size = 576
model_dim = 768
train = 0

# [debug]
# dim1 = 1
# dim2 = 576
# dim3 = 768  

[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# # block 11

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

# # block 12

[layernorm]
input_size = 576
model_dim = 768
train = 0
                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=576
h=12
w=64 

[layernorm]
input_size = 576
model_dim = 768
train = 0

[linear]
input_size = 576
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 576
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=576
h=12
w=64 

