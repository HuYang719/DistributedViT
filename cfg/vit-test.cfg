[net]
batch=1
height=384
width=384
channels=3
model_dim = 768
subdivisions = 1
scales=.01,.01


[convolutional]
batch_normalize=0
filters =768
size = 16
stride = 16
activation=linear

[flatten]
model_dim=768
gw=24
gh=24

[positional_embedding]
input_size=577
model_dim=768

# block 1

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 




# block 2
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 
  

# block 3
                                                       
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 

# # block 4

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 

# # block 5

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 


# # block 6
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 


# # block 7

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 


# # block 8

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 

# # block 9
[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 

# # block 10

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 

# # # block 11

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 


# # # block 12

[layernorm]
input_size = 577
model_dim = 768
train = 0

                                                       
[attention]
head_num = 12
key_dim = 64

[shortcut]
from=-3
c=577
h=12
w=64 

[layernorm]
input_size = 577
model_dim = 768
train = 0

[linear]
input_size = 577
model_dim = 768
output_size = 3072
activation = gelu
dropout = 0

[linear]
input_size = 577
model_dim = 3072
output_size = 768
activation = linear
dropout = 0.1

[shortcut]
from=-4
c=577
h=12
w=64 


# OUTPUT

[layernorm]
input_size = 577
model_dim = 768
train = 0
cut = 1

# [debug]
# dim1 = 1
# dim2 = 1
# dim3 = 768
[linear]
input_size = 1
model_dim = 768
output_size = 1000
activation = softmax
dropout = 0







